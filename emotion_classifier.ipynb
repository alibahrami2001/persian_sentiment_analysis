{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96504b7d",
   "metadata": {},
   "source": [
    "# Emotion Classification using Transformer Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed820d6e",
   "metadata": {},
   "source": [
    "##  Importing Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200e963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bfa43",
   "metadata": {},
   "source": [
    "##  Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7662a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Snappfood - Sentiment Analysis.csv\", delimiter=\"\\t\")\n",
    "df.drop(columns=[\"Unnamed: 0\", \"label_id\"], inplace=True)\n",
    "happy_mask = df[\"label\"] == \"HAPPY\"\n",
    "sad_mask = df[\"label\"] == \"SAD\"\n",
    "happy_comments = df[\"comment\"][happy_mask].to_list()\n",
    "happy_labels = [1 for c in happy_comments]\n",
    "sad_comments = df[\"comment\"][sad_mask].to_list()\n",
    "sad_labels = [0 for c in sad_comments]\n",
    "\n",
    "comments = [*happy_comments, *sad_comments]\n",
    "labels = [*happy_labels, *sad_labels]\n",
    "\n",
    "train_comments, test_comments, train_labels, test_labels = train_test_split(\n",
    "    comments, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8508101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# # Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# class Body:\n",
    "#     def __init__(self, tokenizer, model):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, sentences):\n",
    "#         # Tokenize sentences\n",
    "#         encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#         # Compute token embeddings\n",
    "#         with torch.no_grad():\n",
    "#             model_output = model(**encoded_input)\n",
    "\n",
    "#         # Perform pooling. In this case, max pooling.\n",
    "#         sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "#         return sentence_embeddings\n",
    "\n",
    "\n",
    "# body = Body(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf3176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings = []\n",
    "# for c in tqdm(train_comments):\n",
    "#     embedding = body(c)\n",
    "#     train_embeddings.append(embedding)\n",
    "\n",
    "# test_embeddings = []\n",
    "# for c in tqdm(test_comments):\n",
    "#     embedding = body(c)\n",
    "#     test_embeddings.append(embedding)\n",
    "\n",
    "# torch.save(torch.cat(train_embeddings, dim=0), \"train_embeddings.pt\")\n",
    "# torch.save(torch.cat(test_embeddings, dim=0), \"test_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0f4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = torch.load(\"train_embeddings.pt\")\n",
    "test_embeddings = torch.load(\"test_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c46cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisDatase(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx]\n",
    "        label = self.labels[idx]\n",
    "        return embedding, label\n",
    "\n",
    "\n",
    "train_dataset = SentimentAnalysisDatase(train_embeddings, train_labels)\n",
    "test_dataset = SentimentAnalysisDatase(test_embeddings, test_labels)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4ca348",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "head = nn.Sequential(\n",
    "    nn.Linear(384, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2),\n",
    ")\n",
    "head = head.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1808ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(head.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64182e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 109/109 [00:00<00:00, 193.96it/s, mean_loss=0.397]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 227.09it/s, mean_loss=0.364]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 189.83it/s, mean_loss=0.358]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 230.31it/s, mean_loss=0.354]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 191.64it/s, mean_loss=0.346]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 272.62it/s, mean_loss=0.351]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 186.76it/s, mean_loss=0.338]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 263.67it/s, mean_loss=0.347]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 182.96it/s, mean_loss=0.331]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 121.37it/s, mean_loss=0.349]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 182.16it/s, mean_loss=0.324]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 276.09it/s, mean_loss=0.349]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 182.34it/s, mean_loss=0.32]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 254.87it/s, mean_loss=0.349]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 185.94it/s, mean_loss=0.315]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 272.29it/s, mean_loss=0.35]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 181.34it/s, mean_loss=0.31]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 251.58it/s, mean_loss=0.352]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 187.00it/s, mean_loss=0.306]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 259.02it/s, mean_loss=0.347]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 182.59it/s, mean_loss=0.301]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 250.77it/s, mean_loss=0.35] \n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 151.51it/s, mean_loss=0.297]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 231.80it/s, mean_loss=0.349]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 183.39it/s, mean_loss=0.292]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 240.60it/s, mean_loss=0.356]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 186.78it/s, mean_loss=0.287]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 204.45it/s, mean_loss=0.357]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 184.26it/s, mean_loss=0.283]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 256.82it/s, mean_loss=0.357]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 192.75it/s, mean_loss=0.281]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 222.43it/s, mean_loss=0.357]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 188.22it/s, mean_loss=0.277]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 249.59it/s, mean_loss=0.363]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 149.04it/s, mean_loss=0.273]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 229.13it/s, mean_loss=0.363]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 184.17it/s, mean_loss=0.269]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 256.39it/s, mean_loss=0.364]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 180.93it/s, mean_loss=0.265]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 242.22it/s, mean_loss=0.373]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 185.38it/s, mean_loss=0.261]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 257.75it/s, mean_loss=0.37] \n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 176.90it/s, mean_loss=0.259]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 266.46it/s, mean_loss=0.385]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 184.45it/s, mean_loss=0.255]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 221.79it/s, mean_loss=0.376]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 151.25it/s, mean_loss=0.251]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 277.37it/s, mean_loss=0.371]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 183.96it/s, mean_loss=0.246]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 274.41it/s, mean_loss=0.382]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 204.88it/s, mean_loss=0.242]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 285.46it/s, mean_loss=0.384]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 198.94it/s, mean_loss=0.241]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 276.51it/s, mean_loss=0.385]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 201.45it/s, mean_loss=0.238]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 282.06it/s, mean_loss=0.387]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 197.59it/s, mean_loss=0.234]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 286.66it/s, mean_loss=0.39]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 158.01it/s, mean_loss=0.232]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 239.92it/s, mean_loss=0.392]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 193.58it/s, mean_loss=0.227]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 252.45it/s, mean_loss=0.399]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 193.30it/s, mean_loss=0.223]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 229.24it/s, mean_loss=0.398]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 201.04it/s, mean_loss=0.223]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 274.13it/s, mean_loss=0.405]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 192.72it/s, mean_loss=0.218]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 231.50it/s, mean_loss=0.407]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 185.19it/s, mean_loss=0.217]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 237.41it/s, mean_loss=0.402]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 142.54it/s, mean_loss=0.212]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 237.03it/s, mean_loss=0.414]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 186.12it/s, mean_loss=0.212]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 229.46it/s, mean_loss=0.42] \n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 182.28it/s, mean_loss=0.208]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 252.56it/s, mean_loss=0.414]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 180.37it/s, mean_loss=0.204]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 226.48it/s, mean_loss=0.419]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 183.57it/s, mean_loss=0.202]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 258.35it/s, mean_loss=0.429]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 186.28it/s, mean_loss=0.201]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 212.19it/s, mean_loss=0.417]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 148.38it/s, mean_loss=0.197]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 265.81it/s, mean_loss=0.434]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 182.95it/s, mean_loss=0.193]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 244.37it/s, mean_loss=0.439]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 178.81it/s, mean_loss=0.192]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 213.14it/s, mean_loss=0.442]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 175.42it/s, mean_loss=0.192]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 239.71it/s, mean_loss=0.442]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 177.48it/s, mean_loss=0.186]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 211.49it/s, mean_loss=0.446]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 170.95it/s, mean_loss=0.183]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 230.06it/s, mean_loss=0.444]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 143.06it/s, mean_loss=0.183]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 242.51it/s, mean_loss=0.451]\n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 180.44it/s, mean_loss=0.185]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 219.41it/s, mean_loss=0.45] \n",
      "Train: 100%|██████████| 109/109 [00:00<00:00, 180.80it/s, mean_loss=0.179]\n",
      "Test: 100%|██████████| 28/28 [00:00<00:00, 246.86it/s, mean_loss=0.448]\n"
     ]
    }
   ],
   "source": [
    "# embeddings, labels = next(iter(train_loader))\n",
    "for epoch in range(50):\n",
    "    loss_sum = 0.0\n",
    "    counter = 0\n",
    "    for embeddings, labels in (pbar := tqdm(train_loader, desc=\"Train\")):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = head(embeddings)\n",
    "        loss = loss_function(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        counter += 1\n",
    "\n",
    "        mean_loss = loss_sum / counter\n",
    "\n",
    "        pbar.set_postfix({\"mean_loss\": mean_loss})\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    counter = 0\n",
    "    for embeddings, labels in (pbar := tqdm(test_loader, desc=\"Test\")):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = head(embeddings)\n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        counter += 1\n",
    "\n",
    "        mean_loss = loss_sum / counter\n",
    "\n",
    "        pbar.set_postfix({\"mean_loss\": mean_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c259fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = happy_comments[:3]\n",
    "\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n",
    "print(sentence_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
